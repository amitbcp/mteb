{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a59c92-88f1-402d-88f5-5c01f0d0ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/mnt/shared/aamita/project/image_retrieval/VLM2Vec/\")\n",
    "os.environ[\"MTEB_CACHE\"]=\"/Users/aamita/Oracle/oracle/devops/multimodal_retrieval/image/amit\"\n",
    "os.environ[\"HF_TOKEN\"]=\"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\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2498b688-7ced-44ca-8d1f-f5636237360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vlm_backbone.qwen2_vl import Qwen2VLForConditionalGeneration\n",
    "from src.vlm_backbone.qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor\n",
    "from src.vlm_backbone.qwen2_vl.image_processing_qwen2_vl import Qwen2VLImageProcessor\n",
    "from src.vlm_backbone.qwen2_vl.tokenization_qwen2_fast import Qwen2TokenizerFast\n",
    "from src.vlm_backbone.qwen2_vl import Qwen2VLForConditionalGeneration\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoProcessor #, Qwen2VLForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8cabf4-eec3-43ec-b9da-18a767fd04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mteb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf73e07-23be-435b-9f11-5462da34d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TIGER-Lab/VLM2Vec-Qwen2VL-2B'\n",
    "base_model_name ='Qwen/Qwen2-VL-2B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a058a5d-8162-4a19-9e74-e630f93c5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name='Qwen/Qwen2-VL-2B-Instruct',\n",
    "    checkpoint_path='TIGER-Lab/VLM2Vec-Qwen2VL-2B',\n",
    "    pooling='last',\n",
    "    normalize=True,\n",
    "    model_backbone='qwen2_vl',\n",
    "    lora=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba368036-7812-458d-b1ac-ebafa8f4a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f58cfff8-1c90-46a7-b1db-5be886579290",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_point = \"/mnt/shared/aamita/project/image_retrieval/VLM2Vec/runs/test/mmeb-qwen-test-data-all/checkpoint-1000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61e07f43-0c84-45b5-ba0f-ff42911bb6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            ft_point,\n",
    "            config=config,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4d1cf2-d3f8-45da-a078-61289ac58ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLForConditionalGeneration(\n",
       "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2VLVisionBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): VisionFlashAttention2(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): VisionMlp(\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (act): QuickGELUActivation()\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): PatchMerger(\n",
       "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2VLModel(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "        (self_attn): Qwen2VLFlashAttention2(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb5620-d092-42c9-9a1a-3483cd15684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mteb_model_meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_retrieval",
   "language": "python",
   "name": "image_retrieval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
