{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafe40ad-c7e2-4a24-bb8b-0aff081c5fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-04-30 14:56:56,779:jax._src.xla_bridge:909: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/shared/aamita/project/image_retrieval/mteb/mteb/__init__.py'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/Users/aamita/Oracle/oracle/devops/VLM2Vec\")\n",
    "sys.path.append(\"/mnt/shared/aamita/project/image_retrieval/VLM2Vec\")\n",
    "# os.environ[\"MTEB_CACHE\"]=\"/Users/aamita/Oracle/oracle/devops/multimodal_retrieval/image/amit\"\n",
    "os.environ[\"HF_TOKEN\"]=\"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\"\n",
    "# os.environ[\"HF_HOME\"]=\"/Volumes/LaCie/Lacie_backup/hf/\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import mteb\n",
    "# from mteb.task_selection import results_to_dataframe\n",
    "\n",
    "mteb.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d060e575-3053-4292-b043-aee050187df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='qwen25_7b_bm25' revision='0_1_10' release_date='2024-10-08' languages=['eng-Latn'] loader=functools.partial(<class 'mteb.models.qwen25_model.Qwen25BM25Wrapper'>, model_name='qwen25_7b_bm25', model_path='Qwen/Qwen2.5-VL-7B-Instruct') n_parameters=None memory_usage_mb=None max_tokens=131072.0 embed_dim=5120 license='apache-2.0' open_weights=True public_training_code='' public_training_data='https://github.com/xhluca/bm25s' framework=['PyTorch'] reference='https://github.com/xhluca/bm25s' similarity_fn_name=None use_instructions=True training_datasets=None adapted_from=None superseded_by=None is_cross_encoder=None modalities=['image', 'text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2025 14:57:10 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714cc654c03746dc9ccda0fb6e83250a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"qwen25_7b_bm25\"\n",
    "meta = mteb.get_model_meta(model_name)\n",
    "print(meta)\n",
    "model = mteb.get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bfde5d-fb51-46df-881a-a389a9a8eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = mteb.get_tasks(\n",
    "        languages=[\"eng\"], modalities=[\"text\", \"image\"], task_types=[\"DocumentUnderstanding\",\"VisionCentericQA\"]\n",
    "    )\n",
    "task_names = [task.metadata.name for task in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1245cdd3-0560-4f30-9df2-6c114f60abd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* VidoreArxivQARetrieval*****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2025 14:57:15 - INFO - mteb.evaluation.MTEB -   \n",
      "\n",
      "## Evaluating 1 tasks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #262626; text-decoration-color: #262626\">───────────────────────────────────────────────── </span><span style=\"font-weight: bold\">Selected tasks </span><span style=\"color: #262626; text-decoration-color: #262626\"> ─────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;235m───────────────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[38;5;235m ─────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">DocumentUnderstanding</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mDocumentUnderstanding\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - VidoreArxivQARetrieval, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2i</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - VidoreArxivQARetrieval, \u001b[3;38;5;241mt2i\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2025 14:57:15 - INFO - mteb.evaluation.MTEB -   \n",
      "\n",
      "********************** Evaluating VidoreArxivQARetrieval **********************\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "04/30/2025 14:57:17 - INFO - datasets.builder -   Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/queries/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "04/30/2025 14:57:17 - INFO - datasets.info -   Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/queries/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "Found cached dataset arxivqa_test_subsampled_beir (/home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/queries/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4)\n",
      "04/30/2025 14:57:17 - INFO - datasets.builder -   Found cached dataset arxivqa_test_subsampled_beir (/home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/queries/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4)\n",
      "Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/queries/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "04/30/2025 14:57:17 - INFO - datasets.info -   Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/queries/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "Loading cached processed dataset at /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/queries/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4/cache-5f96447c3909584e.arrow\n",
      "04/30/2025 14:57:17 - INFO - datasets.arrow_dataset -   Loading cached processed dataset at /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/queries/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4/cache-5f96447c3909584e.arrow\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "04/30/2025 14:57:18 - INFO - datasets.builder -   Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/corpus/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "04/30/2025 14:57:18 - INFO - datasets.info -   Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/corpus/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "Found cached dataset arxivqa_test_subsampled_beir (/home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/corpus/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4)\n",
      "04/30/2025 14:57:18 - INFO - datasets.builder -   Found cached dataset arxivqa_test_subsampled_beir (/home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/corpus/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4)\n",
      "Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/corpus/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "04/30/2025 14:57:18 - INFO - datasets.info -   Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/corpus/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "Loading cached processed dataset at /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/corpus/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4/cache-b699bc8b7ff267c7.arrow\n",
      "04/30/2025 14:57:18 - INFO - datasets.arrow_dataset -   Loading cached processed dataset at /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/corpus/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4/cache-b699bc8b7ff267c7.arrow\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "04/30/2025 14:57:19 - INFO - datasets.builder -   Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/qrels/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "04/30/2025 14:57:19 - INFO - datasets.info -   Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/qrels/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "Found cached dataset arxivqa_test_subsampled_beir (/home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/qrels/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4)\n",
      "04/30/2025 14:57:19 - INFO - datasets.builder -   Found cached dataset arxivqa_test_subsampled_beir (/home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/qrels/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4)\n",
      "Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/qrels/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "04/30/2025 14:57:19 - INFO - datasets.info -   Loading Dataset info from /home/aamita/.cache/huggingface/datasets/vidore___arxivqa_test_subsampled_beir/qrels/0.0.0/7d94d570960eac2408d3baa7a33f9de4822ae3e4\n",
      "04/30/2025 14:57:19 - INFO - mteb.abstasks.Image.AbsTaskAny2AnyRetrieval -   Subset: default\n",
      "04/30/2025 14:57:19 - INFO - mteb.models.qwen25_model -   Corpus Modality : image\n",
      "04/30/2025 14:57:19 - INFO - mteb.models.qwen25_model -   Query Modality : text\n",
      "04/30/2025 14:57:19 - INFO - mteb.models.qwen25_model -   Preparing Corpus...\n",
      "04/30/2025 14:57:19 - INFO - mteb.models.qwen25_model -   Encoding Corpus in batches... Warning: This might take a while!\n",
      "04/30/2025 14:57:19 - INFO - mteb.models.qwen25_model -   Scoring Function:)\n",
      "Processing images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [37:43<00:00,  4.53s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8379b59e7475414eaf24c7e5cd6cad63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709da36643114d729965b176683c0329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2025 15:35:03 - DEBUG - bm25s -   Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cd928ee61c45ecbdebeda848c894d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674fd7f1ffa94ecf841171af99dfe1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2025 15:35:03 - INFO - mteb.models.qwen25_model -   Encoding Queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b5304813644e20b3083d0ec1b0bc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058c224f006e40e6871f8ac9e5f9831d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stem Tokens:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2025 15:35:03 - INFO - mteb.models.qwen25_model -   Retrieving Results... 500 queries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0376e086564ff1bc09b62119aaba99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2025 15:35:03 - INFO - mteb.abstasks.Image.AbsTaskAny2AnyRetrieval -   Time taken to retrieve: 2263.76 seconds\n",
      "04/30/2025 15:35:04 - INFO - root -   \n",
      "\n",
      "04/30/2025 15:35:04 - INFO - mteb.evaluation.MTEB -   Evaluation for VidoreArxivQARetrieval on test took 2264.65 seconds\n",
      "04/30/2025 15:35:04 - INFO - mteb.evaluation.MTEB -   Scores: {'default': {'ndcg_at_1': 0.43, 'ndcg_at_3': 0.48717, 'ndcg_at_5': 0.49784, 'ndcg_at_10': 0.51003, 'ndcg_at_20': 0.52326, 'ndcg_at_100': 0.55385, 'ndcg_at_1000': 0.57835, 'map_at_1': 0.43, 'map_at_3': 0.47367, 'map_at_5': 0.47957, 'map_at_10': 0.48453, 'map_at_20': 0.48821, 'map_at_100': 0.49211, 'map_at_1000': 0.4931, 'recall_at_1': 0.43, 'recall_at_3': 0.526, 'recall_at_5': 0.552, 'recall_at_10': 0.59, 'recall_at_20': 0.642, 'recall_at_100': 0.812, 'recall_at_1000': 1.0, 'cv_recall_at_1': 0.43, 'cv_recall_at_3': 0.526, 'cv_recall_at_5': 0.552, 'cv_recall_at_10': 0.59, 'cv_recall_at_20': 0.642, 'cv_recall_at_100': 0.812, 'cv_recall_at_1000': 1.0, 'precision_at_1': 0.43, 'precision_at_3': 0.17533, 'precision_at_5': 0.1104, 'precision_at_10': 0.059, 'precision_at_20': 0.0321, 'precision_at_100': 0.00812, 'precision_at_1000': 0.001, 'mrr_at_1': 0.43, 'mrr_at_3': 0.4736666666666667, 'mrr_at_5': 0.47956666666666664, 'mrr_at_10': 0.484534126984127, 'mrr_at_20': 0.4882126620111914, 'mrr_at_100': 0.49211758630775565, 'mrr_at_1000': 0.4930938132566721, 'nauc_ndcg_at_1_max': np.float64(0.7655876500500709), 'nauc_ndcg_at_1_std': np.float64(0.32921928444161214), 'nauc_ndcg_at_1_diff1': np.float64(0.8195107359769025), 'nauc_ndcg_at_3_max': np.float64(0.7515690180881713), 'nauc_ndcg_at_3_std': np.float64(0.37928076659639776), 'nauc_ndcg_at_3_diff1': np.float64(0.7577569262887436), 'nauc_ndcg_at_5_max': np.float64(0.7474246735162561), 'nauc_ndcg_at_5_std': np.float64(0.3724484608283203), 'nauc_ndcg_at_5_diff1': np.float64(0.752764110763319), 'nauc_ndcg_at_10_max': np.float64(0.7429552791014304), 'nauc_ndcg_at_10_std': np.float64(0.37071395550953684), 'nauc_ndcg_at_10_diff1': np.float64(0.7480367402098411), 'nauc_ndcg_at_20_max': np.float64(0.7384705076900941), 'nauc_ndcg_at_20_std': np.float64(0.3719272301918043), 'nauc_ndcg_at_20_diff1': np.float64(0.7452915316196624), 'nauc_ndcg_at_100_max': np.float64(0.7353220690196411), 'nauc_ndcg_at_100_std': np.float64(0.3756316078717975), 'nauc_ndcg_at_100_diff1': np.float64(0.7407518303728158), 'nauc_ndcg_at_1000_max': np.float64(0.7429785755804258), 'nauc_ndcg_at_1000_std': np.float64(0.3690469182222618), 'nauc_ndcg_at_1000_diff1': np.float64(0.7537656465831205), 'nauc_map_at_1_max': np.float64(0.7655876500500709), 'nauc_map_at_1_std': np.float64(0.32921928444161214), 'nauc_map_at_1_diff1': np.float64(0.8195107359769025), 'nauc_map_at_3_max': np.float64(0.7545190208983553), 'nauc_map_at_3_std': np.float64(0.3670396202504193), 'nauc_map_at_3_diff1': np.float64(0.7724702800538074), 'nauc_map_at_5_max': np.float64(0.7523056562550778), 'nauc_map_at_5_std': np.float64(0.36336878953041013), 'nauc_map_at_5_diff1': np.float64(0.7697431565083515), 'nauc_map_at_10_max': np.float64(0.7507300934148428), 'nauc_map_at_10_std': np.float64(0.3631030672560391), 'nauc_map_at_10_diff1': np.float64(0.7678367698077012), 'nauc_map_at_20_max': np.float64(0.7496084011558443), 'nauc_map_at_20_std': np.float64(0.36341662101287125), 'nauc_map_at_20_diff1': np.float64(0.7671923196923832), 'nauc_map_at_100_max': np.float64(0.7491696312222886), 'nauc_map_at_100_std': np.float64(0.3633443287444644), 'nauc_map_at_100_diff1': np.float64(0.7666910820217061), 'nauc_map_at_1000_max': np.float64(0.7494368142501528), 'nauc_map_at_1000_std': np.float64(0.36315357728338554), 'nauc_map_at_1000_diff1': np.float64(0.7671569075329132), 'nauc_recall_at_1_max': np.float64(0.7655876500500709), 'nauc_recall_at_1_std': np.float64(0.32921928444161214), 'nauc_recall_at_1_diff1': np.float64(0.8195107359769025), 'nauc_recall_at_3_max': np.float64(0.7430073998231486), 'nauc_recall_at_3_std': np.float64(0.41565762061328443), 'nauc_recall_at_3_diff1': np.float64(0.7141992832875695), 'nauc_recall_at_5_max': np.float64(0.732128181746541), 'nauc_recall_at_5_std': np.float64(0.3987835905749378), 'nauc_recall_at_5_diff1': np.float64(0.7007188129735331), 'nauc_recall_at_10_max': np.float64(0.7154650975097949), 'nauc_recall_at_10_std': np.float64(0.39137746342465535), 'nauc_recall_at_10_diff1': np.float64(0.6832934685312634), 'nauc_recall_at_20_max': np.float64(0.6923579925185838), 'nauc_recall_at_20_std': np.float64(0.39845435342161045), 'nauc_recall_at_20_diff1': np.float64(0.6650479368406886), 'nauc_recall_at_100_max': np.float64(0.6350646465073957), 'nauc_recall_at_100_std': np.float64(0.4641534587468517), 'nauc_recall_at_100_diff1': np.float64(0.5713439278836995), 'nauc_recall_at_1000_max': nan, 'nauc_recall_at_1000_std': nan, 'nauc_recall_at_1000_diff1': nan, 'nauc_precision_at_1_max': np.float64(0.7655876500500709), 'nauc_precision_at_1_std': np.float64(0.32921928444161214), 'nauc_precision_at_1_diff1': np.float64(0.8195107359769025), 'nauc_precision_at_3_max': np.float64(0.7430073998231482), 'nauc_precision_at_3_std': np.float64(0.4156576206132842), 'nauc_precision_at_3_diff1': np.float64(0.7141992832875688), 'nauc_precision_at_5_max': np.float64(0.7321281817465407), 'nauc_precision_at_5_std': np.float64(0.3987835905749376), 'nauc_precision_at_5_diff1': np.float64(0.7007188129735327), 'nauc_precision_at_10_max': np.float64(0.715465097509795), 'nauc_precision_at_10_std': np.float64(0.3913774634246554), 'nauc_precision_at_10_diff1': np.float64(0.683293468531263), 'nauc_precision_at_20_max': np.float64(0.692357992518584), 'nauc_precision_at_20_std': np.float64(0.3984543534216099), 'nauc_precision_at_20_diff1': np.float64(0.6650479368406874), 'nauc_precision_at_100_max': np.float64(0.6350646465073964), 'nauc_precision_at_100_std': np.float64(0.464153458746853), 'nauc_precision_at_100_diff1': np.float64(0.5713439278837001), 'nauc_precision_at_1000_max': np.float64(1.0), 'nauc_precision_at_1000_std': np.float64(1.0), 'nauc_precision_at_1000_diff1': np.float64(1.0), 'nauc_cv_recall_at_1_max': np.float64(0.7655876500500709), 'nauc_cv_recall_at_1_std': np.float64(0.32921928444161214), 'nauc_cv_recall_at_1_diff1': np.float64(0.8195107359769025), 'nauc_cv_recall_at_3_max': np.float64(0.7430073998231486), 'nauc_cv_recall_at_3_std': np.float64(0.41565762061328443), 'nauc_cv_recall_at_3_diff1': np.float64(0.7141992832875695), 'nauc_cv_recall_at_5_max': np.float64(0.732128181746541), 'nauc_cv_recall_at_5_std': np.float64(0.3987835905749378), 'nauc_cv_recall_at_5_diff1': np.float64(0.7007188129735331), 'nauc_cv_recall_at_10_max': np.float64(0.7154650975097949), 'nauc_cv_recall_at_10_std': np.float64(0.39137746342465535), 'nauc_cv_recall_at_10_diff1': np.float64(0.6832934685312634), 'nauc_cv_recall_at_20_max': np.float64(0.6923579925185838), 'nauc_cv_recall_at_20_std': np.float64(0.39845435342161045), 'nauc_cv_recall_at_20_diff1': np.float64(0.6650479368406886), 'nauc_cv_recall_at_100_max': np.float64(0.6350646465073957), 'nauc_cv_recall_at_100_std': np.float64(0.4641534587468517), 'nauc_cv_recall_at_100_diff1': np.float64(0.5713439278836995), 'nauc_cv_recall_at_1000_max': nan, 'nauc_cv_recall_at_1000_std': nan, 'nauc_cv_recall_at_1000_diff1': nan, 'nauc_mrr_at_1_max': np.float64(0.7655876500500709), 'nauc_mrr_at_1_std': np.float64(0.32921928444161214), 'nauc_mrr_at_1_diff1': np.float64(0.8195107359769025), 'nauc_mrr_at_3_max': np.float64(0.7545190208983553), 'nauc_mrr_at_3_std': np.float64(0.3670396202504193), 'nauc_mrr_at_3_diff1': np.float64(0.7724702800538074), 'nauc_mrr_at_5_max': np.float64(0.7523056562550778), 'nauc_mrr_at_5_std': np.float64(0.36336878953041013), 'nauc_mrr_at_5_diff1': np.float64(0.7697431565083515), 'nauc_mrr_at_10_max': np.float64(0.7507300934148428), 'nauc_mrr_at_10_std': np.float64(0.3631030672560391), 'nauc_mrr_at_10_diff1': np.float64(0.7678367698077012), 'nauc_mrr_at_20_max': np.float64(0.7496084011558443), 'nauc_mrr_at_20_std': np.float64(0.36341662101287125), 'nauc_mrr_at_20_diff1': np.float64(0.7671923196923832), 'nauc_mrr_at_100_max': np.float64(0.7491653860178058), 'nauc_mrr_at_100_std': np.float64(0.36333556932766187), 'nauc_mrr_at_100_diff1': np.float64(0.766682934169955), 'nauc_mrr_at_1000_max': np.float64(0.7494233936212271), 'nauc_mrr_at_1000_std': np.float64(0.3631571357379272), 'nauc_mrr_at_1000_diff1': np.float64(0.7671249226217562), 'main_score': 0.49784}}\n"
     ]
    }
   ],
   "source": [
    "# task_names =[\"VidoreArxivQARetrieval\"]\n",
    "\n",
    "for task in task_names :\n",
    "    print(f\"************* {task}*****************\")\n",
    "    evaluation = mteb.MTEB(tasks=[task])\n",
    "    try :\n",
    "        results = evaluation.run(\n",
    "                model,\n",
    "                save_corpus_embeddings=True,\n",
    "                device=\"auto\",\n",
    "                save_predictions=True,\n",
    "                export_errors=True,\n",
    "                verbosity=2,\n",
    "                encode_kwargs={\"batch_size\": 1},\n",
    "            )\n",
    "    except :\n",
    "        continue\n",
    "\n",
    "    # Sample Runs\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc37aa-c134-44ed-b82a-b5d9eca3f896",
   "metadata": {},
   "source": [
    "# Next Set is Any2AnyRetrieval\n",
    "\n",
    "Any2AnyRetrieval is expensive, we verify that previous dataset was completed and then we sort the datasets in terms of importance and size which is subset of original Any2AnyRetrieval  and run. But be careful of the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd49e4-1635-441a-87e2-8a2e700e0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = mteb.get_tasks(\n",
    "        languages=[\"eng\"], modalities=[\"text\", \"image\"], task_types=[\"Any2AnyRetrieval\"]\n",
    "    )\n",
    "task_names = [task.metadata.name for task in tasks]\n",
    "\n",
    "\n",
    "for task in task_names :\n",
    "    print(f\"************* {task}*****************\")\n",
    "    evaluation = mteb.MTEB(tasks=[task])\n",
    "    try :\n",
    "        results = evaluation.run(\n",
    "                model,\n",
    "                save_corpus_embeddings=True,\n",
    "                device=\"auto\",\n",
    "                save_predictions=True,\n",
    "                export_errors=True,\n",
    "                verbosity=2,\n",
    "                encode_kwargs={\"batch_size\": 1},\n",
    "            )\n",
    "    except :\n",
    "        continue\n",
    "\n",
    "    # Sample Runs\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aca7bd-43ec-47e0-b08f-e68b77aee1c6",
   "metadata": {},
   "source": [
    "# Double Check if Anything is missed\n",
    "\n",
    "Runs which are successfull will not run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9d5a1-3289-4d48-a07e-3b20ede6a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = [\n",
    " 'VidoreSyntheticDocQAEnergyRetrieval'\n",
    " 'VidoreTabfquadRetrieval',\n",
    " 'VidoreSyntheticDocQAGovernmentReportsRetrieval',\n",
    " 'VidoreSyntheticDocQAAIRetrieval',\n",
    " 'VidoreInfoVQARetrieval',\n",
    " 'VidoreTatdqaRetrieval',\n",
    " 'VidoreShiftProjectRetrieval',\n",
    " 'VidoreArxivQARetrieval',\n",
    " 'VidoreSyntheticDocQAHealthcareIndustryRetrieval',\n",
    " 'VidoreDocVQARetrieval',\n",
    "\n",
    " 'BLINKIT2TMultiChoice',\n",
    " 'BLINKIT2IMultiChoice',\n",
    " 'CVBenchDepth',\n",
    " 'CVBenchCount',\n",
    " 'CVBenchRelation',\n",
    " 'CVBenchDistance',\n",
    "    \n",
    " 'BLINKIT2TRetrieval',\n",
    " 'WebQAT2TRetrieval',\n",
    " 'MemotionI2TRetrieval',\n",
    " 'HatefulMemesI2TRetrieval',\n",
    " 'ROxfordMediumI2IRetrieval',\n",
    " 'RParisMediumI2IRetrieval',\n",
    " 'Flickr30kI2TRetrieval',\n",
    "\n",
    "\n",
    " # Might be Repeatitive But Evaluation will skip anything that is already done\n",
    " 'BLINKIT2IMultiChoice',\n",
    " 'BLINKIT2TMultiChoice',\n",
    " 'BLINKIT2TRetrieval',\n",
    " 'CVBenchCount',\n",
    " 'CVBenchDepth',\n",
    " 'CVBenchRelation',\n",
    " 'Fashion200kI2TRetrieval',\n",
    " 'Flickr30kI2TRetrieval',\n",
    " 'HatefulMemesI2TRetrieval',\n",
    " 'MemotionI2TRetrieval',\n",
    " 'MemotionT2IRetrieval',\n",
    " 'OKVQAIT2TRetrieval',\n",
    " 'ROxfordMediumI2IRetrieval',\n",
    " 'RParisMediumI2IRetrieval',\n",
    " 'SciMMIRI2TRetrieval',\n",
    " 'SciMMIRT2IRetrieval',\n",
    " 'VidoreArxivQARetrieval',\n",
    " 'VidoreDocVQARetrieval',\n",
    " 'VidoreInfoVQARetrieval',\n",
    " 'VidoreShiftProjectRetrieval',\n",
    " 'VidoreSyntheticDocQAAIRetrieval',\n",
    " 'VidoreSyntheticDocQAEnergyRetrieval',\n",
    " 'VidoreSyntheticDocQAHealthcareIndustryRetrieval',\n",
    " 'VidoreTabfquadRetrieval',\n",
    " 'VidoreTatdqaRetrieval',\n",
    " 'VisualNewsI2TRetrieval',\n",
    " 'VizWizIT2TRetrieval',\n",
    " 'WebQAT2TRetrieval'\n",
    " 'MSCOCOI2TRetrieval',\n",
    " 'LLaVAIT2TRetrieval',\n",
    "    \n",
    " \n",
    " # Optional/If resources exist\n",
    " 'FashionIQIT2IRetrieval',\n",
    " 'FORBI2IRetrieval',\n",
    " 'RP2kI2IRetrieval',\n",
    " 'ImageCoDeT2IRetrieval',\n",
    " 'ROxfordHardI2IRetrieval',\n",
    " 'StanfordCarsI2IRetrieval',\n",
    " 'MemotionT2IRetrieval',\n",
    " 'SciMMIRT2IRetrieval',\n",
    " 'RParisEasyI2IRetrieval',\n",
    " 'TUBerlinT2IRetrieval',\n",
    " 'ROxfordEasyI2IRetrieval',\n",
    " 'Flickr30kT2IRetrieval',\n",
    " 'ReMuQIT2TRetrieval',\n",
    " 'NIGHTSI2IRetrieval',\n",
    " 'HatefulMemesT2IRetrieval',\n",
    " 'BLINKIT2IRetrieval',\n",
    " 'RParisHardI2IRetrieval',\n",
    " 'CUB200I2IRetrieval',\n",
    " 'CIRRIT2IRetrieval',\n",
    " 'OVENIT2TRetrieval',\n",
    " 'MSCOCOT2IRetrieval',\n",
    " 'SciMMIRI2TRetrieval',\n",
    " 'VisualNewsI2TRetrieval',\n",
    " 'OKVQAIT2TRetrieval',\n",
    " 'VizWizIT2TRetrieval',\n",
    " \n",
    "# 'GLDv2I2TRetrieval', \n",
    "# 'InfoSeekIT2TRetrieval',\n",
    "# 'Fashion200kI2TRetrieval',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e2f3a-c7c0-4556-8a9b-35b7bce27ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in task_names :\n",
    "    print(f\"************* {task}*****************\")\n",
    "    evaluation = mteb.MTEB(tasks=[task])\n",
    "    try :\n",
    "        results = evaluation.run(\n",
    "                model,\n",
    "                save_corpus_embeddings=True,\n",
    "                device=\"auto\",\n",
    "                save_predictions=True,\n",
    "                export_errors=True,\n",
    "                verbosity=2,\n",
    "                encode_kwargs={\"batch_size\": 1},\n",
    "            )\n",
    "    except :\n",
    "        continue\n",
    "\n",
    "    # Sample Runs\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62180bf6-9ed5-4dd1-8e99-11e5997ab577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad22fd-e747-49c5-84e5-5a0881647638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd74c7-1750-4c72-97ce-c841bb20c573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef8996ab-c5d3-4bd9-acb7-639e4cb4671b",
   "metadata": {},
   "source": [
    "# Qwen2.5-VL HF Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd656493-22bc-4b12-8daf-7bf42fe122be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's highly recommanded to use `[decord]` feature for faster video loading.\n",
    "!pip install qwen-vl-utils[decord]==0.0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf00131-c2c2-4561-841f-6456762ffddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574e9829-613d-442f-b5eb-b2ef2db476bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2491946e7c804eb6ac05a929cafe828c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af42e4579cd5407fa474654ed739963a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default: Load the model on the available device(s)\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83daf5c6-1676-41c8-b3fc-4e56a7ebb5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9113a347f2f4505a37e6d4b2f6aa676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea29e709-342d-4433-9e7c-c580628a0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                # \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "                \"image\":img_data_uri,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "173beda5-2dc0-4f75-aa5e-eed4d7b65461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d17049da-6bc8-4dc6-8649-8c644d133f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff728e95-a8c4-4766-942a-6aa2b37258e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The image shows a person wearing a light-colored, short-sleeved button-up shirt with a subtle pattern. The shirt has a collar and a small logo on the left chest area. The background appears to be an indoor setting with a blurred view of what looks like a window or glass partition, suggesting an office environment. The lighting is soft and even, highlighting the person's features without creating harsh shadows.\"]\n"
     ]
    }
   ],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfa764-b7ff-4f09-ba98-f86d117f87ca",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e95512-2bdb-405d-87c1-d547e80b3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "pil_image = Image.open('amit.png')\n",
    "# Save the PIL image to a bytes buffer\n",
    "buffered = io.BytesIO()\n",
    "pil_image.save(buffered, format=\"PNG\")  # You can also use \"JPEG\"\n",
    "img_bytes = buffered.getvalue()\n",
    "\n",
    "# Encode to base64\n",
    "img_base64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "# Optionally create a full data URI\n",
    "img_data_uri = f\"data:image/png;base64,{img_base64}\"\n",
    "# return img_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26de2f-7421-4e22-9eb5-fa9a5b2b34cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_retrieval",
   "language": "python",
   "name": "image_retrieval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
